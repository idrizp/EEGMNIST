{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEG - MNIST project.\n",
    "\n",
    "**Author:** Idriz Pelaj\n",
    "\n",
    "We use the data provided kindly by the people over at [MindBigData](https://mindbigdata.com/opendb/index.html) in order to train a convolutional neural network on the spectrograms of EEG data so that we can sort of map what MNIST numbers people are looking at and thinking about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision numpy Pillow matplotlib requests scipy opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database Helpers\n",
    "Some functions here will be used as database helpers for our sqlite3 database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sqlite3\n",
    "import io\n",
    "\n",
    "# Helper functions to help with converting numpy arrays to buffers that we can write to the database, and from buffers to arrays that we can load from the database.\n",
    "\n",
    "\"\"\"\n",
    "Converts a numpy array to bytes.\n",
    "\"\"\"\n",
    "def numpy_arr_to_bytes(arr):\n",
    "    buf = io.BytesIO()\n",
    "    np.save(buf, arr)\n",
    "    buf.seek(0) # Go back to 0\n",
    "    return buf.read() # Read the result.\n",
    "\n",
    "\"\"\"\n",
    "Convert bytes to a numpy array.\n",
    "\"\"\"\n",
    "def bytes_to_numpy_arr(bytes):\n",
    "    buf = io.BytesIO(bytes)\n",
    "    return np.load(buf)\n",
    "\n",
    "def db_connect():\n",
    "    return sqlite3.connect('spectrograms.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Time Setup\n",
    "This is mostly concerned with setting up things such as the spectrogram database that we will be using to train our neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining and preparing the MW data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we download the MW data from the official URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "url = \"https://mindbigdata.com/opendb/MindBigData-MW-v1.0.zip\" # We are using the MindWave data - it's less work, and should work as a baseline for testing the model architecture.\n",
    "response = requests.get(url)\n",
    "response.raise_for_status() # This will let us assert the request was successful.\n",
    "\n",
    "with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "    z.extractall(\"mw_data\") # Unzips and extracts to our mw_data folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to actually parse this data. Rather trivial, as the file format is given on the official MindBigData website - and since we're working with MindWave data, this actually becomes a lot easier, too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "Reads the data from the MindWave.\n",
    "Returns a tuple of (code, values)\n",
    "\"\"\"\n",
    "def read_data(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        rows = f.readlines()\n",
    "        rows_data = []\n",
    "        for row in rows:\n",
    "            code = int(row.split(\"\\t\")[4])\n",
    "            values = np.array([int(x) for x in row.split(\"\\t\")[6].split(\",\")])\n",
    "            rows_data.append((code, values))\n",
    "        return rows_data\n",
    "\n",
    "raw_eeg_data = read_data(\"./mw_data/MW.txt\") # This will give us a tuple array of (code, values).\n",
    "[x[1].shape for x in raw_eeg_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can immediately see is that the number of rows for each EEG reading is not exactly the same. We get around this by \"forcing\" a certain resolution later on, which is not optimal for the spectral characteristics for the signal, but from my tests it didn't seem to have too much of an impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the signals for clarity\n",
    "We'll plot a few of the signals we're working with for clarity. It should simply serve as a reminder of what we're working with, and also plots are nice! In the mean time, we'll define some helper functions for the data - namely, finding the first occurrence of a \"code\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Finds the first \"code\" occurrence in the MW data.\n",
    "\"\"\"\n",
    "def find_first_code_occurrence(rows, code):\n",
    "        for row in rows:\n",
    "            if row[0] == code:\n",
    "                return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the raw EEG signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# This will plot all the data necessary.\n",
    "for l in range(-1, 9):\n",
    "        code, data = find_first_code_occurrence(raw_eeg_data, l)\n",
    "        t = np.arange(0, len(data))\n",
    "\n",
    "        plt.plot(t, data, label=f\"Number: {code}\")\n",
    "        plt.xlabel(\"n\")\n",
    "        plt.ylabel(\"raw_eeg_data[n] amplitude(in µV)\")\n",
    "        plt.ylim([-250, 500])\n",
    "        plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks quite noisy! The amplitude is in $\\mu V$, and we index by $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Power Spectral Density(PSD) of these signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fun, we can also see the PSD of these signals - what we'll notice is that the average frequency range(in CT) fluctuates in an interval around ±200Hz. The sampling frequency $f_s$ is given at around ~512Hz, which means our nyquist frequency is around ~256Hz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import welch\n",
    "\n",
    "f_s = 512 # The MindWave samples at around 512Hz.\n",
    "\n",
    "for l in range(-1, 9):\n",
    "    code, data = find_first_code_occurrence(raw_eeg_data, l)\n",
    "    frequencies, psd = welch(data, f_s, nperseg=1024) # We compute the FFT using welch's algorithm.\n",
    "\n",
    "    # Plot the PSD\n",
    "    plt.semilogy(frequencies, psd)\n",
    "    plt.grid()\n",
    "    plt.title('Power Spectral Density')\n",
    "    plt.xlabel('Frequency [Hz]')\n",
    "    plt.ylabel('PSD [V**2/Hz]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the power spectral density as obtained using [Welch's algorithm](https://en.wikipedia.org/wiki/Welch%27s_method)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the spectrogram of a signal.\n",
    "Here, we'll plot the spectrogram of _one_ signal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import ShortTimeFFT\n",
    "def plot_spectrogram(data, code, f_s): # A helper method to plot the spectrogram.\n",
    "    data = np.array(data)\n",
    "\n",
    "    win = ('gaussian', 1e-2 * f_s) # A gaussian window should be fine.\n",
    "    N = len(data)\n",
    "    \n",
    "    t = np.arange(N) / f_s\n",
    "    nperseg, noverlap = 50, 40 # These parameters seem to work fine in obtaining a good spectrogram.\n",
    "\n",
    "    SFT = ShortTimeFFT.from_window(win, f_s, nperseg, noverlap, fft_mode='centered', scale_to='magnitude', phase_shift=None)\n",
    "\n",
    "    Sz1 = SFT.stft(data) \n",
    "    plt.figure()\n",
    "\n",
    "    t_lo, t_hi, f_lo, f_hi = SFT.extent(N, center_bins=True)\n",
    "\n",
    "    plt.ylim(f_lo, f_hi)\n",
    "    plt.xlim(t_lo, t_hi)\n",
    "    plt.title(f\"Number: {code}\")\n",
    "    plt.xlabel(rf\"Time $t$ in seconds ($\\Delta t= {SFT.delta_t:g}\\,$s)\")\n",
    "\n",
    "    extent = SFT.extent(N, center_bins=True)\n",
    "\n",
    "    kw = dict(origin='lower', aspect='auto', cmap='viridis')\n",
    "    im1b = plt.imshow(abs(Sz1), extent=extent, **kw)\n",
    "    plt.colorbar(im1b, label=\"Magnitude $|S_z(t, f)|$\")\n",
    "    _ = plt.ylabel(r\"Frequency $f$ in Hertz ($\\Delta f = %g\\,$Hz)\" %\n",
    "                   SFT.delta_f, x=0.08, y=0.5, fontsize='medium')\n",
    "\n",
    "first_elem_code, first_elem_data = find_first_code_occurrence(raw_eeg_data, 1)\n",
    "plot_spectrogram(first_elem_data, 0, f_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty cool, no? The MindBigData website also does say that the duration is around 2 seconds, so we further verify that we have done everything correctly so far! Of course - do recall this is the amplitude representation. Phase isn't really necessary in my experience, but if it is, please be sure to let me know!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the spectrograms of all the data and saving to a SQLite database.\n",
    "We are now going to prepare the spectrograms of all of the data we have, reshape it - and store it as numpy blobs in our database. This will make it easier to make this data a little more portable, or in our case, not flood our filesystem with npy files!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inserting the data into the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to open a database and create the necessary table for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and create the necessary database and table.\n",
    "with db_connect() as conn:\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Create the table.\n",
    "    cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS spectrograms (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        code INTEGER,\n",
    "        array BLOB NOT NULL\n",
    "    )\n",
    "    ''')\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we obtain all the spectrograms of our data and insert them into the database. We have a helper function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Returns the spectrogram of the raw EEG data given.\n",
    "Returns: (Sz1, (t_lo, t_hi), (f_lo, f_hi), code)\n",
    "\"\"\"\n",
    "def get_spectrogram(data, code, f_s):\n",
    "    data = np.array(data)\n",
    "\n",
    "    win = ('gaussian', 1e-2 * f_s)\n",
    "    N = len(data)\n",
    "\n",
    "    nperseg, noverlap = 50, 40\n",
    "    SFT = ShortTimeFFT.from_window(win, f_s, nperseg, noverlap, fft_mode='centered', scale_to='magnitude', phase_shift=None)\n",
    "\n",
    "    Sz1 = SFT.stft(data)\n",
    "\n",
    "    t_lo, t_hi, f_lo, f_hi = SFT.extent(N, center_bins=True)\n",
    "    return (Sz1, (t_lo, t_hi), (f_lo, f_hi), code)\n",
    "\n",
    "# This will pad all data so that there's 110 columns. This should be fine.\n",
    "def pad_to_110_cols(spectrogram_data):\n",
    "    cols_to_pad = 110 - spectrogram_data.shape[1]\n",
    "    # Use np.pad to add zeros to the right side of the array\n",
    "    padded_array = np.pad(spectrogram_data, ((0, 0), (0, cols_to_pad)), mode='constant')\n",
    "    return padded_array\n",
    "\n",
    "with db_connect() as conn:\n",
    "    cursor = conn.cursor()\n",
    "    for code, data in raw_eeg_data:\n",
    "        amp_arr, (t_lo, t_hi), (f_lo, f_hi), code  = get_spectrogram(data, code, f_s) # Obtains the spectrograms. This will take a bit to compute.\n",
    "        amp_arr = np.abs(amp_arr).round().astype(np.uint16) # We're working with power magnitude - no negatives, and we should probably stick to integer values, so we can save on some space.\n",
    "        amp_arr = pad_to_110_cols(amp_arr) # Now we have 110 columns. We are always ensured this is the maximum number of columns, as well as knowing that the maximum number of rows we have is 50.\n",
    "        bytes = numpy_arr_to_bytes(amp_arr)\n",
    "        cursor.execute('INSERT INTO spectrograms(array, code) VALUES (?, ?)', (bytes, code))\n",
    "    conn.commit()\n",
    "    print(f\"Database updated.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "The main idea behind the neural network is the use of a convolutional neural network. We'll be using PyTorch for it. Initially, this project was built on my own neural network library that I spun up in order to learn more about neural networks, which will be found and referenced on my github later on - however it is far too un-optimized for a project that does this much training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data from the spectrogram database\n",
    "In order to get started, we must first extract the data from the spectrogram database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Fetches the spectrograms from the database, pretty straightforward.\n",
    "def fetch_spectrograms(database_path):\n",
    "    conn = sqlite3.connect(database_path)\n",
    "    cursor = conn.cursor()\n",
    "    query = \"SELECT array, code FROM spectrograms\"\n",
    "    cursor.execute(query)\n",
    "    rows = cursor.fetchall()\n",
    "    conn.close()\n",
    "    \n",
    "    return [[bytes_to_numpy_arr(array), code] for array, code in rows]\n",
    "\n",
    "spectrograms_data = fetch_spectrograms(\"spectrograms.db\")\n",
    "print(\"Number of entries:\", len(spectrograms_data))\n",
    "print(\"Example spectrogram entry shape:\", spectrograms_data[0][0].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing the data\n",
    "We should normalize the amplitudes first, row-by-row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(0, len(spectrograms_data)):\n",
    "    data = spectrograms_data[i][0]\n",
    "    spectrograms_data[i][0] = data / np.max(data) # Normalize row-by-row. \n",
    "\n",
    "print(\"Maximum value of first row:\", np.max(spectrograms_data[0][0]))\n",
    "print(\"Minimum value of first row:\", np.min(spectrograms_data[0][0]))\n",
    "print(\"Shape of first row:\", spectrograms_data[0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "The model is a fairly standard convolutional neural network - we'll slowly learn more and more features of the \"images\"(which are actually arrays that have been normalized from 0 to 1, with only one channel). We'll build it using pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's update the PyTorch device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Get cpu, gpu or mps device for training. This was taken directly from pytorch.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "torch.device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, the testing and train split methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data, train_percentile=0.8, batch_size=32):\n",
    "    count = int(len(data) * train_percentile)\n",
    "    train = data[:count]\n",
    "    testing = data[count:]\n",
    "\n",
    "    x_train = torch.tensor([x[0] for x in train], dtype=torch.float32, device=device).unsqueeze(1)\n",
    "    print(x_train.shape)\n",
    "    y_train = torch.tensor([x[1] + 1 for x in train], dtype=torch.long, device=device) # we don't want negative indices\n",
    "\n",
    "    train_data = TensorDataset(x_train, y_train)\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    x_test = torch.tensor([x[0] for x in testing], dtype=torch.float32, device=device).unsqueeze(1)\n",
    "    y_test = torch.tensor([x[1] + 1 for x in testing], dtype=torch.long, device=device) # we don't want negative indices\n",
    "\n",
    "    test_data = TensorDataset(x_test, y_test)\n",
    "    test_loader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "train_loader, test_loader = train_test_split(spectrograms_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The CNN\n",
    "We'll define a pretty conventional convolutional neural network model that has three convolutional layers(as well as does subsampling in the meanwhile), and eventually maps it to 11 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EEGCNN, self).__init__()\n",
    "        self.conv_stack = nn.Sequential(\n",
    "            # First convolutional step\n",
    "            nn.Conv2d(1, 32, 3), # (48, 108)\n",
    "            nn.MaxPool2d(2, 2), # Subsample (48, 108) --> (24, 54)\n",
    "            nn.ReLU(), # Apply non-linearity\n",
    "            \n",
    "            # Second convolutional step\n",
    "            nn.Conv2d(32, 64, 3), # (24, 54)\n",
    "            nn.MaxPool2d(2, 2), # (24, 54) --> (11, 26)\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # Third convolutional step\n",
    "            nn.Conv2d(64, 128, 3), # (11, 26)\n",
    "            nn.MaxPool2d(2, 2), # (11, 26) --> (4, 12)\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Flatten(), # (N, 128 * 4 * 12)\n",
    "\n",
    "            nn.Linear(128 * 4 * 12, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(128, 11)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.conv_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = EEGCNN().to(device)\n",
    "\n",
    "# Test out the shape of the output of the model.\n",
    "test_input = torch.randn(32, 1, 50, 110).type(torch.float32).to(device)\n",
    "model(test_input).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer): # Taken from the PyTorch docs.\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_loader, model, loss_fn, optimizer)\n",
    "    test(test_loader, model, loss_fn)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model\n",
    "We'll save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
